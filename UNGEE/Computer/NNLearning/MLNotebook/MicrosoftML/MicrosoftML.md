### 基本工作原理
* 输入input： $x$
* 权重weights： $w$
* 偏移bias：使 $wx$ 到达临界值 $y，y=wx+b$
* 求和sum： $Z=\displaystyle\sum_{i=1}^m(w_i\cdot x_i)+b或Z=W\cdot X+b$
* 激活activation： $A=\sigma(Z)$
  * Tip:
  * 一个神经元可多个输入
  * 只能一个b和输出
  * 可以同时输出给多个神经元
  * w与输入数量一致
  * w和b要设初始值，训练过程中会不断被修改
  * 允许A=Z，但 $\sigma()$ 必有
  * 一层的所有神经元 $\sigma()$ 必须一致
* 回归/拟合（Regression/Fitting）：给x输出y，y与样本数据形成的曲线的距离最小
* 分类
### 梯度下降
* 是矢量
* 在loss基础上靠近损失最小的点，调整权重**方向**
* $\nabla$是场论中的符号,是矢量微分算符，定义为 $\nabla=\frac{d}{dr}$ 。 高等数学中的梯度,散度,旋度都会用到这个算符。 其二阶导数中旋度的散度又称Laplace算符
* $\theta_{n+1}=\theta_n- \eta\cdot\nabla J(\theta)$
### 前向计算
* **初始化**到**损失函数前**的计算过程
### 损失函数
* 单个样本为误差函数，多个为损失
* 均方差损失函数（回归）
* 交叉熵损失函数（分类）
### 反向传播
* 把损失值反向对应的传给NN的每一层，让每一层根据损失值反向调整权重
  
### 线性反向传播
* 链式法则：因式分解，偏导，传递性， $\Delta z = n\cdot{} \Delta x$

（存疑）
归一化：
* $x'=\frac{x-mian(x)}{max(x)-min(x)}$
* 没有改变数据分布形状

标准化：
* $x'=\frac{x-\mu}{\sigma}$(均值和标准差)
* 将数据分布近似为某种分布（如高斯分布）










